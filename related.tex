Recently, there has been an increasing interest in declarative and data-centric
languages. MapReduce~\cite{Dean:2008:MSD:1327452.1327492}, for instance, is a
popular data-centric programming model that is optimized for large clusters. The
scheduling and data sharing model is very simple: in the \emph{map phase}, data
is transformed at each node and the result reduced to a final result in the
\emph{reduce phase}. Like LM, many programming systems model the program as a
graph where computation will be performed. The Dryad
system~\cite{Isard:2007:DDD:1272996.1273005} combines computational vertices
with communication channels (edges) to form a data-flow graph. The program is
scheduled to run on multiple computers or cores and data is partitioned
automatically during runtime.

Many programming languages follow the so-called \emph{coordination
paradigm}~\cite{Papadopoulos98coordinationmodels}, a form of distributed
programming that divides execution in two parts: \emph{computation}, where the actual
computation is performed, and \emph{coordination}, which deals with
communication and cooperation between processing units. This paradigm attempts
to clearly distinguish between these two parts by providing abstractions for
coordination in an attempt to provide architecture and system-independent forms
of communication.  

Linda~\cite{linda} is probably the most famous coordination model. Linda
implements a data-driven coordination model and features a \emph{tuple space}
that can be manipulated using the following coordination directives:
\texttt{out(t)} writes a tuple \texttt{t} into the tuple space; \texttt{in(t)}
reads a tuple using the template \texttt{t}; \texttt{rd(t)} retrieves a copy of
the tuple \texttt{t} from the tuple space; and \texttt{eval(p)} puts a process
\texttt{p} in the tuple space and executes it in parallel. 
Linda is be implemented on top of many
popular languages by simply creating a communication and storage mechanism for
the tuple space and then adding the directives as a language library.

Another coordination language is Delirium~\cite{Delirium}, where instead of being
embbeded in another language like Linda, it actually embeds operators other
languages inside Delirium. The advantages of Delirium are improved abstraction
and easier debugging because sequential operators are isolated from the
coordination language.

Linda and Delirium are limited in the sense that the programmer can only
coordinate the scheduling of processing units, while the placement of data is
left to the implementation. LM also differs from those languages since it
raises the abstraction level from the processing units to nodes of a graph,
which are part of the program logic.
Furthermore, the language LM is both the coordination and the computation
language and there is no distinction between them.

\iffalse
GraphLab~\cite{GraphLab2010} is a C++ framework for developing parallel machine
learning algorithms. GraphLab allows nodes to
have read/write access to different scopes through different concurrent access
models in order to balance performance and data consistency. While some programs
only need to access the local node's data, others may need to update edge
information. Each consistency model will provide different guarantees that are
better adapted to some algorithms. GraphLab provides different schedulers
that dictate the order in which node's are computed, which is a rudimentary form
of coordination. Later in this paper, we will show how certain GraphLab's
schedulers can be easily implemented in LM through the use of coordination
facts.
\fi
